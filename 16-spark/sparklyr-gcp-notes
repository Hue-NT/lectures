
dataproc clusters create sparklyr --master-machine-type n1-standard-4 --master-boot-disk-size 50 --num-workers 2 --worker-machine-type n1-standard-4 --worker-boot-disk-size 50 --image-version 1.3-ubuntu18

compute firewall-rules create allow-sparklyr-m --allow=tcp:8787

compute ssh sparklyr-m

compute ssh sparklyr-w-0
compute ssh sparklyr-w-1

gcloud dataproc clusters delete sparklyr 

gsutil -ls
gsutil rm -r gs://dataproc-temp-us-west1-388186427283-s7w5smhq/ 


sudo sh -c 'echo "deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/" >> /etc/apt/sources.list' \
&& sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 \
&& sudo apt update && sudo apt upgrade -y \
&& sudo apt install -y r-base r-base-dev

sudo apt install -y libcurl4-openssl-dev libssl-dev libxml2-dev

sudo add-apt-repository -y ppa:ubuntugis/ubuntugis-unstable \
&& sudo apt update && sudo apt upgrade -y \
&& sudo apt -y install libgeos-dev libproj-dev libgdal-dev libudunits2-dev


sudo adduser rstudio

sudo apt install -y gdebi-core \
&& wget https://download2.rstudio.org/server/bionic/amd64/rstudio-server-1.2.5033-amd64.deb \
&& gdebi rstudio-server-1.2.5033-amd64.deb ## hit "y" when prompted

gcloud compute ssh \
    sparklyr-m -- \
    -L 8787:localhost:8787

passphrase: test sparklyr 1


#  Add the PPA
sudo add-apt-repository -y "ppa:marutter/rrutter3.5" \
&& sudo add-apt-repository -y "ppa:marutter/c2d4u3.5" \
&& sudo apt update

# To list all of the available binaries:
apt-cache search r-cran- | sort | less

# Or, to see if a specific package is available:
apt-cache search r-cran | grep sparklyr

# Install your package(s) of choice, e.g.:
sudo apt install -y r-cran-sparklyr
sudo apt install -y r-cran-sf



----


library(sparklyr)

sparklyr::spark_install(version = "2.3")

spark_installed_versions()

library(sparklyr)
library(dplyr)
spark_home_set()
Sys.setenv(HADOOP_CONF_DIR = '/etc/hadoop/conf')
Sys.setenv(YARN_CONF_DIR = '/etc/hadoop/conf')


sc <- spark_connect(master = "yarn-client")

# echo "spark.hadoop.yarn.timeline-service.enabled false" \
# >> $SPARK_HOME/conf/spark-defaults.conf

spark_connection_is_open(sc)

install.packages("nycflights13")
flights_tbl <- copy_to(sc, nycflights13::flights, "flights")

flights_tbl %>%
  select(carrier, dep_delay) %>%
  group_by(carrier) %>%
  summarize(count = n(), mean_dep_delay = mean(dep_delay)) %>%
  arrange(desc(mean_dep_delay))


iris <- copy_to(sc, datasets::iris, "iris")
iris %>%
  spark_apply(nrow, group_by = "Species")
iris %>%
  spark_apply(
    function(e) summary(lm(Petal_Length ~ Petal_Width, e))$r.squared,
    names = "r.squared",
    group_by = "Species")
purrr::map(c("versicolor", "virginica", "setosa"),
           ~dplyr::filter(datasets::iris, Species == !!.x) %>%
             ggplot2::ggplot(ggplot2::aes(x = Petal.Length, y = Petal.Width)) +
             ggplot2::geom_point())

grid <- list(minsplit = c(2, 5, 10), maxdepth = c(1, 3, 8)) %>%
  purrr:::cross_df() %>%
  copy_to(sc, ., repartition = 9, "grid")
grid

spark_apply(
  grid,
  function(grid, cars) {
    model <- rpart::rpart(
      am ~ hp + mpg,
      data = cars,
      control = rpart::rpart.control(minsplit = grid$minsplit,
                                     maxdepth = grid$maxdepth)
    )
    dplyr::mutate(
      grid,
      accuracy = mean(round(predict(model, dplyr::select(cars, -am))) == cars$am)
    )
  },
  context = mtcars)


install.packages("geospark")

library(geospark)
# library(sparklyr)
# library(dplyr)

polygons <- system.file("examples/polygons.txt", package="geospark") %>%
  read.table(sep="|", col.names = c("area", "geom"))

points <- system.file("examples/points.txt", package="geospark") %>%
  read.table(sep = "|", col.names = c("city", "state", "geom"))

polygons_wkt <- copy_to(sc, polygons, "polygons")
points_wkt <- copy_to(sc, points, "points")

polygons_wkt <- mutate(polygons_wkt, y = st_geomfromwkt(geom))
points_wkt <- mutate(points_wkt, x = st_geomfromwkt(geom))

inner_join(polygons_wkt,
           points_wkt,
           sql_on = sql("st_contains(y,x)")) %>% 
  group_by(area, state) %>%
  summarise(cnt = n()) 



spark_disconnect_all()
